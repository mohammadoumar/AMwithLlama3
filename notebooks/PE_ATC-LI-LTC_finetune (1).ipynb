{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oHFCsV0z-Jw"
   },
   "source": [
    "# Finetune PE dataset for LI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr7rB3szzhtx"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "giM74oK1rRIH",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %cd ..\n",
    "# %rm -rf LLaMA-Factory\n",
    "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "# %cd LLaMA-Factory\n",
    "# %ls\n",
    "# !pip install -e .[torch,bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y pydantic\n",
    "# !pip install pydantic==1.10.9 # \n",
    "\n",
    "# !pip uninstall -y gradio\n",
    "# !pip install gradio==3.48.0\n",
    "\n",
    "# !pip uninstall -y bitsandbytes\n",
    "# !pip install --upgrade bitsandbytes\n",
    "\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Restart kernel afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    \n",
    "    assert torch.cuda.is_available() is True\n",
    "    \n",
    "except AssertionError:\n",
    "    \n",
    "    print(\"Please set up a GPU before using LLaMA Factory...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeYs5Lz-QJYk"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** MODEL NAME ***\n",
    "\n",
    "# base_model = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "\n",
    "with open(\"tmp.pkl\", \"rb\") as fh:\n",
    "        \n",
    "        l = pickle.load(fh)\n",
    "        base_model = l[0]\n",
    "        train_dataset_name = l[1]\n",
    "        test_dataset_name = l[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "\n",
    "# *** TRAIN DATASET NAME *** #\n",
    "\n",
    "# train_dataset_name = \"PE_LI_train.json\"\n",
    "train_dataset_file = os.path.join(dataset_dir, train_dataset_name)\n",
    "\n",
    "# *** TEST DATASET NAME *** #\n",
    "\n",
    "# test_dataset_name = \"PE_LI_test.json\"\n",
    "test_dataset_file = os.path.join(dataset_dir, test_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgR3UFhB0Ifq"
   },
   "source": [
    "## Fine-tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *** MODEL DIR ***\n",
    "model_name = f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{base_model.split(\"/\")[1]}\"\"\"\n",
    "\n",
    "train_file = os.path.join(os.getcwd(), f\"\"\"cli_files/{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{base_model.split(\"/\")[1]}.json\"\"\")\n",
    "output_dir = os.path.join(os.getcwd(), \"models\", model_name)\n",
    "\n",
    "nb_epochs = 10 # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/cli_files/PE_LI_llama-3-70b-Instruct-bnb-4bit.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_info_line =  {\n",
    "  \"file_name\": f\"{train_dataset_file}\",\n",
    "  \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\"\n",
    "  }\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': '/Utilisateurs/umushtaq/datasets/PE_LI_train.json',\n",
       " 'columns': {'prompt': 'instruction', 'query': 'input', 'response': 'output'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_info_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(\"LLaMA-Factory/data/dataset_info.json\", 'r+') as fh:\n",
    "#     file_data = json.load(fh)\n",
    "#     file_data[\"persuasive_essays\"] = dataset_info_line\n",
    "#     fh.seek(0)\n",
    "#     json.dump(file_data, fh, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LLaMA-Factory/data/dataset_info.json\", \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "\n",
    "data[\"persuasive_essays\"] = dataset_info_line\n",
    "\n",
    "with open(\"LLaMA-Factory/data/dataset_info.json\", \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CS0Qk5OR0i4Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dict(\n",
    "  stage=\"sft\",                           # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "  model_name_or_path=base_model,         # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=\"persuasive_essays\",           # use alpaca and identity datasets\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=output_dir,                 # the path to save LoRA adapters\n",
    "  overwrite_output_dir=True,             # overrides existing output contents\n",
    "  per_device_train_batch_size=2,         # the batch size\n",
    "  gradient_accumulation_steps=4,         # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",            # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=3000,                       # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                    # the learning rate\n",
    "  num_train_epochs=nb_epochs,            # the epochs of training\n",
    "  max_samples=2000,                       # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  quantization_bit=4,                    # use 4-bit QLoRA\n",
    "  loraplus_lr_ratio=16.0,                # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                             # use float16 mixed precision training\n",
    "  report_to=\"none\"                       # discards wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(args, open(train_file, \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory/\n",
    "!set train_file = train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/cli_files/PE_LI_llama-3-70b-Instruct-bnb-4bit.json'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/05/2024 10:36:38 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "06/05/2024 10:36:38 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-05 10:36:38,226 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-05 10:36:38,227 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-05 10:36:38,227 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-05 10:36:38,227 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-05 10:36:38,584 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "06/05/2024 10:36:38 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "06/05/2024 10:36:38 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/datasets/PE_LI_train.json...\n",
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 14646, 902, 5727, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 10765, 5811, 4398, 1990, 5811, 6956, 304, 279, 14646, 13, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 13840, 304, 2768, 3645, 25, 18305, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 61453, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 595, 2595, 14711, 5810, 374, 279, 14646, 1495, 25, 330, 2181, 374, 2744, 1071, 430, 10937, 649, 13750, 12192, 279, 4500, 315, 8752, 662, 763, 2015, 311, 18167, 304, 279, 10937, 1174, 5220, 3136, 311, 7417, 872, 3956, 323, 2532, 1174, 323, 439, 264, 1121, 1174, 279, 4459, 8396, 463, 2203, 388, 662, 4452, 1174, 994, 584, 4358, 279, 4360, 315, 10937, 477, 23915, 1174, 1148, 584, 527, 11920, 922, 374, 539, 279, 4459, 8396, 1174, 719, 279, 4500, 315, 459, 3927, 364, 274, 4459, 2324, 662, 5659, 420, 1486, 315, 1684, 1174, 358, 32620, 4510, 430, 366, 1741, 16, 11, 17559, 46644, 29, 584, 1288, 15866, 810, 12939, 311, 23915, 2391, 6156, 6873, 694, 1741, 16, 11, 17559, 46644, 29, 662, 702, 257, 128009, 128006, 78191, 128007, 271, 1318, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### You are an expert in Argument Mining. You are given a paragraph which contains argument components enclosed by <AC></AC> tags. Your task is to identify argument relations between argument components in the paragraph. You must return a list of argument component pairs in following format: [(target AC (int), source AC (int)),..., (target AC (int), source AC (int))]\n",
      "\n",
      "### Here is the paragraph text: \"It is always said that competition can effectively promote the development of economy. In order to survive in the competition, companies continue to improve their products and service, and as a result, the whole society prospers. However, when we discuss the issue of competition or cooperation, what we are concerned about is not the whole society, but the development of an individual's whole life. From this point of view, I firmly believe that <AC1, MajorClaim> we should attach more importance to cooperation during primary education </AC1, MajorClaim>.\"\n",
      "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[]<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1318, 128009]\n",
      "labels:\n",
      "[]<|eot_id|>\n",
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-05 10:36:39,230 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-05 10:36:39,231 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-70b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "06/05/2024 10:36:39 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "[WARNING|quantization_config.py:393] 2024-06-05 10:36:39,312 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3474] 2024-06-05 10:36:39,315 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-05 10:36:39,325 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-05 10:36:39,327 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [04:11<00:00, 41.89s/it]\n",
      "[INFO|modeling_utils.py:4280] 2024-06-05 10:40:52,054 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-05 10:40:52,055 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-70b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-05 10:40:52,237 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-05 10:40:52,237 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "06/05/2024 10:40:52 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/05/2024 10:40:52 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/05/2024 10:40:52 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/05/2024 10:40:52 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/05/2024 10:40:52 - INFO - llamafactory.model.utils.misc - Found linear modules: gate_proj,v_proj,q_proj,k_proj,up_proj,o_proj,down_proj\n",
      "06/05/2024 10:40:54 - INFO - llamafactory.model.loader - trainable params: 103546880 || all params: 70657253376 || trainable%: 0.1465\n",
      "[INFO|trainer.py:641] 2024-06-05 10:40:54,403 >> Using auto half precision backend\n",
      "06/05/2024 10:40:54 - WARNING - llamafactory.extras.callbacks - Previous trainer log in this folder will be deleted.\n",
      "06/05/2024 10:40:54 - INFO - llamafactory.train.utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "[INFO|trainer.py:2078] 2024-06-05 10:40:54,738 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-05 10:40:54,738 >>   Num examples = 500\n",
      "[INFO|trainer.py:2080] 2024-06-05 10:40:54,738 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:2081] 2024-06-05 10:40:54,738 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2084] 2024-06-05 10:40:54,738 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-05 10:40:54,738 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2086] 2024-06-05 10:40:54,739 >>   Total optimization steps = 620\n",
      "[INFO|trainer.py:2087] 2024-06-05 10:40:54,753 >>   Number of trainable parameters = 103,546,880\n",
      "{'loss': 3.226, 'grad_norm': nan, 'learning_rate': 3.225806451612903e-06, 'epoch': 0.16}\n",
      "{'loss': 0.6351, 'grad_norm': 2.1450061798095703, 'learning_rate': 1.129032258064516e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1453, 'grad_norm': 0.578214704990387, 'learning_rate': 1.935483870967742e-05, 'epoch': 0.48}\n",
      "{'loss': 0.1122, 'grad_norm': 0.28447863459587097, 'learning_rate': 2.7419354838709678e-05, 'epoch': 0.64}\n",
      "{'loss': 0.1146, 'grad_norm': 0.9580300450325012, 'learning_rate': 3.548387096774194e-05, 'epoch': 0.8}\n",
      "{'loss': 0.1063, 'grad_norm': 0.1578788161277771, 'learning_rate': 4.3548387096774194e-05, 'epoch': 0.96}\n",
      "{'loss': 0.0542, 'grad_norm': 0.8698496222496033, 'learning_rate': 4.99984151186201e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0666, 'grad_norm': 0.2944815456867218, 'learning_rate': 4.994296536700177e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0498, 'grad_norm': 0.2381005436182022, 'learning_rate': 4.980847237981281e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0711, 'grad_norm': 0.39044588804244995, 'learning_rate': 4.9595362359820727e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0646, 'grad_norm': 0.1747967153787613, 'learning_rate': 4.930431064394977e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0524, 'grad_norm': 1.2636983394622803, 'learning_rate': 4.8936239563165895e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0729, 'grad_norm': 0.25630924105644226, 'learning_rate': 4.849231551964771e-05, 'epoch': 2.08}\n",
      "{'loss': 0.044, 'grad_norm': 0.9223984479904175, 'learning_rate': 4.7973945290505766e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0653, 'grad_norm': 0.19100305438041687, 'learning_rate': 4.7382771569763485e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0396, 'grad_norm': 0.31995198130607605, 'learning_rate': 4.67206677627271e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0434, 'grad_norm': 1.8670862913131714, 'learning_rate': 4.598973204924097e-05, 'epoch': 2.72}\n",
      "{'loss': 0.0581, 'grad_norm': 0.23322553932666779, 'learning_rate': 4.5192280734641626e-05, 'epoch': 2.88}\n",
      "{'loss': 0.0608, 'grad_norm': 0.466427206993103, 'learning_rate': 4.433084090948099e-05, 'epoch': 3.04}\n",
      "{'loss': 0.0316, 'grad_norm': 0.5307861566543579, 'learning_rate': 4.340814244127993e-05, 'epoch': 3.2}\n",
      "{'loss': 0.0296, 'grad_norm': 0.22763404250144958, 'learning_rate': 4.242710932368998e-05, 'epoch': 3.36}\n",
      "{'loss': 0.0238, 'grad_norm': 0.06124374642968178, 'learning_rate': 4.139085041047711e-05, 'epoch': 3.52}\n",
      "{'loss': 0.0291, 'grad_norm': 0.2619401812553406, 'learning_rate': 4.030264956369157e-05, 'epoch': 3.68}\n",
      "{'loss': 0.0596, 'grad_norm': 0.3867473602294922, 'learning_rate': 3.916595524724353e-05, 'epoch': 3.84}\n",
      "{'loss': 0.0298, 'grad_norm': 0.32813844084739685, 'learning_rate': 3.798436959886219e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0163, 'grad_norm': 0.20136550068855286, 'learning_rate': 3.67616370150689e-05, 'epoch': 4.16}\n",
      "{'loss': 0.022, 'grad_norm': 0.35245245695114136, 'learning_rate': 3.5501632285337875e-05, 'epoch': 4.32}\n",
      "{'loss': 0.0195, 'grad_norm': 0.06873007118701935, 'learning_rate': 3.420834831304718e-05, 'epoch': 4.48}\n",
      "{'loss': 0.0264, 'grad_norm': 0.4389353394508362, 'learning_rate': 3.2885883462131394e-05, 'epoch': 4.64}\n",
      "{'loss': 0.0226, 'grad_norm': 0.05284583568572998, 'learning_rate': 3.153842856953417e-05, 'epoch': 4.8}\n",
      "{'loss': 0.0195, 'grad_norm': 0.008215083740651608, 'learning_rate': 3.0170253664617686e-05, 'epoch': 4.96}\n",
      "{'loss': 0.0089, 'grad_norm': 0.039733532816171646, 'learning_rate': 2.878569443761442e-05, 'epoch': 5.12}\n",
      "{'loss': 0.0121, 'grad_norm': 0.013104138895869255, 'learning_rate': 2.738913850000246e-05, 'epoch': 5.28}\n",
      "{'loss': 0.0055, 'grad_norm': 0.34326156973838806, 'learning_rate': 2.598501148034439e-05, 'epoch': 5.44}\n",
      "{'loss': 0.0166, 'grad_norm': 0.013290343806147575, 'learning_rate': 2.4577762999651726e-05, 'epoch': 5.6}\n",
      "{'loss': 0.0245, 'grad_norm': 0.30556026101112366, 'learning_rate': 2.3171852570718097e-05, 'epoch': 5.76}\n",
      "{'loss': 0.0137, 'grad_norm': 0.33349305391311646, 'learning_rate': 2.177173546610597e-05, 'epoch': 5.92}\n",
      "{'loss': 0.0058, 'grad_norm': 0.0209675133228302, 'learning_rate': 2.0381848599570276e-05, 'epoch': 6.08}\n",
      "{'loss': 0.0033, 'grad_norm': 0.04060908034443855, 'learning_rate': 1.9006596465660548e-05, 'epoch': 6.24}\n",
      "{'loss': 0.0066, 'grad_norm': 0.02464882656931877, 'learning_rate': 1.7650337182058084e-05, 'epoch': 6.4}\n",
      "{'loss': 0.0035, 'grad_norm': 0.040655963122844696, 'learning_rate': 1.6317368678879495e-05, 'epoch': 6.56}\n",
      "{'loss': 0.0164, 'grad_norm': 0.03762015700340271, 'learning_rate': 1.5011915078712251e-05, 'epoch': 6.72}\n",
      "{'loss': 0.0054, 'grad_norm': 0.11198166012763977, 'learning_rate': 1.3738113310543177e-05, 'epoch': 6.88}\n",
      "{'loss': 0.0074, 'grad_norm': 0.052349016070365906, 'learning_rate': 1.2500000000000006e-05, 'epoch': 7.04}\n",
      "{'loss': 0.0019, 'grad_norm': 0.008308791555464268, 'learning_rate': 1.1301498677450037e-05, 'epoch': 7.2}\n",
      "{'loss': 0.0022, 'grad_norm': 0.1131860539317131, 'learning_rate': 1.0146407344493186e-05, 'epoch': 7.36}\n",
      "{'loss': 0.009, 'grad_norm': 0.048669490963220596, 'learning_rate': 9.038386438250415e-06, 'epoch': 7.52}\n",
      "{'loss': 0.0022, 'grad_norm': 0.007564735133200884, 'learning_rate': 7.980947231588471e-06, 'epoch': 7.68}\n",
      "{'loss': 0.0023, 'grad_norm': 0.02222428470849991, 'learning_rate': 6.977440706039973e-06, 'epoch': 7.84}\n",
      "{'loss': 0.0008, 'grad_norm': 0.040543317794799805, 'learning_rate': 6.031046932680229e-06, 'epoch': 8.0}\n",
      "{'loss': 0.001, 'grad_norm': 0.03625575825572014, 'learning_rate': 5.1447649946122e-06, 'epoch': 8.16}\n",
      "{'loss': 0.0003, 'grad_norm': 0.007769430987536907, 'learning_rate': 4.32140348299504e-06, 'epoch': 8.32}\n",
      "{'loss': 0.001, 'grad_norm': 0.007068774662911892, 'learning_rate': 3.5635715967337223e-06, 'epoch': 8.48}\n",
      "{'loss': 0.0006, 'grad_norm': 0.007593849208205938, 'learning_rate': 2.8736708740346146e-06, 'epoch': 8.64}\n",
      "{'loss': 0.0044, 'grad_norm': 0.007439556531608105, 'learning_rate': 2.2538875820292347e-06, 'epoch': 8.8}\n",
      "{'loss': 0.0028, 'grad_norm': 0.011967605911195278, 'learning_rate': 1.7061857885832893e-06, 'epoch': 8.96}\n",
      "{'loss': 0.0004, 'grad_norm': 0.0015932945534586906, 'learning_rate': 1.232301138246042e-06, 'epoch': 9.12}\n",
      "{'loss': 0.0004, 'grad_norm': 0.00407327339053154, 'learning_rate': 8.337353520638469e-07, 'epoch': 9.28}\n",
      "{'loss': 0.0009, 'grad_norm': 0.0033878490794450045, 'learning_rate': 5.117514686876379e-07, 'epoch': 9.44}\n",
      "{'loss': 0.0047, 'grad_norm': 0.004325950518250465, 'learning_rate': 2.6736984185520284e-07, 'epoch': 9.6}\n",
      "{'loss': 0.0017, 'grad_norm': 0.054948896169662476, 'learning_rate': 1.0136490693196665e-07, 'epoch': 9.76}\n",
      "{'loss': 0.0011, 'grad_norm': 0.03737054765224457, 'learning_rate': 1.4262726757049982e-08, 'epoch': 9.92}\n",
      "100%|███████████████████████████████████████| 620/620 [4:03:17<00:00, 23.55s/it][INFO|trainer.py:2329] 2024-06-05 14:44:12,715 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 14597.9617, 'train_samples_per_second': 0.343, 'train_steps_per_second': 0.042, 'train_loss': 0.08999064714580234, 'epoch': 9.92}\n",
      "100%|███████████████████████████████████████| 620/620 [4:03:17<00:00, 23.55s/it]\n",
      "[INFO|trainer.py:3410] 2024-06-05 14:44:12,750 >> Saving model checkpoint to /Utilisateurs/umushtaq/models/PE_LI_llama-3-70b-Instruct-bnb-4bit\n",
      "[INFO|configuration_utils.py:733] 2024-06-05 14:44:13,311 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-05 14:44:13,312 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-05 14:44:17,820 >> tokenizer config file saved in /Utilisateurs/umushtaq/models/PE_LI_llama-3-70b-Instruct-bnb-4bit/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-05 14:44:17,840 >> Special tokens file saved in /Utilisateurs/umushtaq/models/PE_LI_llama-3-70b-Instruct-bnb-4bit/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        9.92\n",
      "  total_flos               = 559208257GF\n",
      "  train_loss               =        0.09\n",
      "  train_runtime            =  4:03:17.96\n",
      "  train_samples_per_second =       0.343\n",
      "  train_steps_per_second   =       0.042\n",
      "[INFO|modelcard.py:450] 2024-06-05 14:44:18,206 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train $train_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVNaC-xS5N40"
   },
   "source": [
    "## Inference on the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/models/PE_LI_llama-3-70b-Instruct-bnb-4bit'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'trainer_log.jsonl',\n",
       " 'README.md',\n",
       " 'adapter_model.safetensors',\n",
       " 'adapter_config.json',\n",
       " 'tokenizer_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'tokenizer.json',\n",
       " 'training_args.bin',\n",
       " 'train_results.json',\n",
       " 'all_results.json',\n",
       " 'trainer_state.json']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "oh8H9A_25SF9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %cd /content/LLaMA-Factory/\n",
    "\n",
    "args = dict(\n",
    "  model_name_or_path=base_model, # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=output_dir,            # load the saved LoRA adapters\n",
    "  template=\"llama3\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2108] 2024-06-05 14:44:20,376 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-05 14:44:20,379 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-05 14:44:20,381 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-05 14:44:20,383 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-05 14:44:20,730 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/05/2024 14:44:20 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-05 14:44:20,884 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-05 14:44:20,887 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-70b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/05/2024 14:44:20 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "06/05/2024 14:44:20 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|quantization_config.py:393] 2024-06-05 14:44:24,328 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3474] 2024-06-05 14:44:24,392 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-05 14:44:24,529 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-05 14:44:24,532 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80702ba956d74e498ea2ba08ef383ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4280] 2024-06-05 14:55:52,245 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-05 14:55:52,247 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-70b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-05 14:55:52,451 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-05 14:55:52,454 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/05/2024 14:55:52 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/05/2024 14:55:52 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/05/2024 14:55:52 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/05/2024 14:56:04 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/models/PE_LI_llama-3-70b-Instruct-bnb-4bit\n",
      "06/05/2024 14:56:04 - INFO - llamafactory.model.loader - all params: 70657253376\n"
     ]
    }
   ],
   "source": [
    "model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(test_dataset_file, \"r+\") as fh:\n",
    "    test_dataset = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prompts = []\n",
    "test_grounds = []\n",
    "\n",
    "for sample in test_dataset:\n",
    "    test_prompts.append(\"\\nUser:\" + sample[\"instruction\"] + sample[\"input\"])\n",
    "    test_grounds.append(sample[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = \"\"\n",
    "    \n",
    "    for new_text in model.stream_chat(messages):\n",
    "        #print(new_text, end=\"\", flush=True)\n",
    "        response += new_text\n",
    "        #print()\n",
    "    test_predictions.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, f\"\"\"PE_LI_results_{nb_epochs}.pickle\"\"\"), 'wb') as fh:\n",
    "    results_d = {\"ground_truths\": test_grounds,\n",
    "                 \"predictions\": test_predictions    \n",
    "        \n",
    "    }\n",
    "    pickle.dump(results_d, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
