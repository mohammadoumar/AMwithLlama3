{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671272c9-0c9d-4265-a75b-480471345729",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165cbdc-bb7a-48d3-b17d-442687c00173",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3de7add-5182-4d9f-af93-5446726c9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e11ea0-ff3f-44eb-9577-5eedba91e3cc",
   "metadata": {},
   "source": [
    "## Run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d31e870a-6452-42fb-9206-19e4e4da5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\", \n",
    "    \"unsloth/llama-3-8b-Instruct\", \n",
    "    # \"unsloth/llama-3-70b-Instruct-bnb-4bit\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2037e51a-9a25-41d6-af79-c740357364a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    [\"PE_ATC_paragraph_train.json\", \"PE_ATC_paragraph_test.json\"],\n",
    "    [\"PE_ATC_paragraph_wo_tags_train.json\", \"PE_ATC_paragraph_wo_tags_test.json\"],\n",
    "    # [\"PE_ATC_essay_train.json\", \"PE_ATC_essay_test.json\"],\n",
    "    # [\"PE_ATC_essay_wo_tags_train.json\", \"PE_ATC_essay_wo_tags_test.json\"]    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdcc75b-8db5-4d15-9f33-1432cec3e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unsloth/llama-3-8b-Instruct-bnb-4bit', 'PE_ATC_paragraph_train.json', 'PE_ATC_paragraph_test.json']\n",
      "\n",
      "Running experiment ['unsloth/llama-3-8b-Instruct-bnb-4bit', 'PE_ATC_paragraph_train.json', 'PE_ATC_paragraph_test.json']\n",
      "unsloth/llama-3-8b-Instruct-bnb-4bit PE_ATC_paragraph_train.json PE_ATC_paragraph_test.json\n",
      "/Utilisateurs/umushtaq/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/12/2024 20:45:22 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "06/12/2024 20:45:22 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-12 20:45:22,982 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-12 20:45:22,982 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-12 20:45:22,982 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-12 20:45:22,982 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-12 20:45:23,240 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "06/12/2024 20:45:23 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "06/12/2024 20:45:23 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/datasets/PE_ATC_paragraph_train.json...\n",
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 14646, 902, 5727, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 4718, 3465, 374, 311, 49229, 1855, 5811, 6956, 304, 279, 14646, 439, 3060, 330, 35575, 46644, 498, 330, 46644, 1, 477, 330, 42562, 1082, 3343, 1472, 2011, 471, 264, 1160, 315, 5811, 3777, 4595, 304, 2768, 4823, 3645, 25, 5324, 8739, 9962, 794, 510, 8739, 1857, 320, 496, 705, 3777, 1857, 320, 496, 705, 61453, 3777, 1857, 320, 496, 7400, 633, 14711, 5810, 374, 279, 14646, 1495, 25, 366, 16816, 29, 12540, 4236, 387, 15972, 311, 20874, 477, 311, 47903, 949, 694, 16816, 29, 128009, 128006, 78191, 128007, 271, 5018, 8739, 9962, 794, 3132, 92, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### You are an expert in Argument Mining. You are given a paragraph which contains argument components enclosed by <AC></AC> tags. Your task is to classify each argument components in the paragraph as either \"MajorClaim\", \"Claim\" or \"Premise\". You must return a list of argument component types in following JSON format: {\"component_types\": [component_type (str), component_type (str),..., component_type (str)]}\n",
      "\n",
      "### Here is the paragraph text: <topic> Should students be taught to compete or to cooperate? </topic><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"component_types\": []}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 8739, 9962, 794, 3132, 92, 128009]\n",
      "labels:\n",
      "{\"component_types\": []}<|eot_id|>\n",
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-12 20:45:23,973 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-12 20:45:23,974 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "06/12/2024 20:45:23 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "[WARNING|quantization_config.py:393] 2024-06-12 20:45:24,042 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3474] 2024-06-12 20:45:24,044 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/model.safetensors\n",
      "[INFO|modeling_utils.py:1519] 2024-06-12 20:45:24,076 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-12 20:45:24,080 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4280] 2024-06-12 20:45:27,306 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-12 20:45:27,306 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-12 20:45:27,584 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/2950abc9d0b34ddd43fd52bbf0d7dca82807ce96/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-12 20:45:27,584 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "06/12/2024 20:45:27 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/12/2024 20:45:27 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/12/2024 20:45:27 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/12/2024 20:45:27 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/12/2024 20:45:27 - INFO - llamafactory.model.utils.misc - Found linear modules: up_proj,k_proj,down_proj,q_proj,v_proj,gate_proj,o_proj\n",
      "06/12/2024 20:45:28 - INFO - llamafactory.model.loader - trainable params: 20971520 || all params: 8051232768 || trainable%: 0.2605\n",
      "[INFO|trainer.py:641] 2024-06-12 20:45:28,042 >> Using auto half precision backend\n",
      "06/12/2024 20:45:28 - INFO - llamafactory.train.utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "[INFO|trainer.py:2078] 2024-06-12 20:45:28,297 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-12 20:45:28,297 >>   Num examples = 1,796\n",
      "[INFO|trainer.py:2080] 2024-06-12 20:45:28,297 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:2081] 2024-06-12 20:45:28,297 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2084] 2024-06-12 20:45:28,297 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-12 20:45:28,297 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2086] 2024-06-12 20:45:28,297 >>   Total optimization steps = 2,240\n",
      "[INFO|trainer.py:2087] 2024-06-12 20:45:28,301 >>   Number of trainable parameters = 20,971,520\n",
      "{'loss': 0.746, 'grad_norm': 4.7610087394714355, 'learning_rate': 2.2321428571428573e-06, 'epoch': 0.04}\n",
      "{'loss': 0.1433, 'grad_norm': 2.314831495285034, 'learning_rate': 4.241071428571429e-06, 'epoch': 0.09}\n",
      "{'loss': 0.0961, 'grad_norm': 1.3178775310516357, 'learning_rate': 6.473214285714287e-06, 'epoch': 0.13}\n",
      "{'loss': 0.0902, 'grad_norm': 1.704479455947876, 'learning_rate': 8.705357142857143e-06, 'epoch': 0.18}\n",
      "{'loss': 0.0812, 'grad_norm': 0.7506862282752991, 'learning_rate': 1.09375e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0715, 'grad_norm': 1.2099156379699707, 'learning_rate': 1.3169642857142858e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0557, 'grad_norm': 0.3937990665435791, 'learning_rate': 1.5401785714285715e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0482, 'grad_norm': 0.7277775406837463, 'learning_rate': 1.7633928571428573e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0792, 'grad_norm': 0.5677917003631592, 'learning_rate': 1.9866071428571427e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0569, 'grad_norm': 1.1984567642211914, 'learning_rate': 2.2098214285714286e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0541, 'grad_norm': 0.8050350546836853, 'learning_rate': 2.4330357142857144e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0439, 'grad_norm': 0.8396134376525879, 'learning_rate': 2.6562500000000002e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0329, 'grad_norm': 0.6954169273376465, 'learning_rate': 2.8794642857142857e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0439, 'grad_norm': 0.14348751306533813, 'learning_rate': 3.102678571428572e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0504, 'grad_norm': 1.1890013217926025, 'learning_rate': 3.325892857142857e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0538, 'grad_norm': 0.41508224606513977, 'learning_rate': 3.5491071428571435e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0558, 'grad_norm': 0.7371333837509155, 'learning_rate': 3.7723214285714286e-05, 'epoch': 0.76}\n",
      "{'loss': 0.051, 'grad_norm': 0.6519221067428589, 'learning_rate': 3.9955357142857144e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0453, 'grad_norm': 1.112366795539856, 'learning_rate': 4.21875e-05, 'epoch': 0.85}\n",
      "{'loss': 0.04, 'grad_norm': 0.3178185522556305, 'learning_rate': 4.4419642857142854e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0435, 'grad_norm': 0.3282867670059204, 'learning_rate': 4.665178571428572e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0748, 'grad_norm': 0.44807571172714233, 'learning_rate': 4.888392857142857e-05, 'epoch': 0.98}\n",
      "{'loss': 0.0532, 'grad_norm': 0.21925149857997894, 'learning_rate': 4.9999241131520337e-05, 'epoch': 1.02}\n",
      "{'loss': 0.0354, 'grad_norm': 0.04095001518726349, 'learning_rate': 4.999317046010329e-05, 'epoch': 1.07}\n",
      "{'loss': 0.021, 'grad_norm': 0.14391638338565826, 'learning_rate': 4.998103059143599e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0644, 'grad_norm': 0.8241804838180542, 'learning_rate': 4.996282447349408e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0345, 'grad_norm': 0.303043007850647, 'learning_rate': 4.9938556527346155e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0457, 'grad_norm': 0.5359107255935669, 'learning_rate': 4.9908232646080166e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0414, 'grad_norm': 0.3193907141685486, 'learning_rate': 4.987186019337242e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0286, 'grad_norm': 0.25655320286750793, 'learning_rate': 4.9829448001699384e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0631, 'grad_norm': 0.5810026526451111, 'learning_rate': 4.9781006370192876e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0372, 'grad_norm': 0.5226894617080688, 'learning_rate': 4.972654706213906e-05, 'epoch': 1.43}\n",
      "{'loss': 0.029, 'grad_norm': 0.40003058314323425, 'learning_rate': 4.966608330212198e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0461, 'grad_norm': 0.2932955026626587, 'learning_rate': 4.9599629772812096e-05, 'epoch': 1.51}\n",
      "{'loss': 0.045, 'grad_norm': 2.205383777618408, 'learning_rate': 4.95272026114009e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0576, 'grad_norm': 2.1522815227508545, 'learning_rate': 4.9448819405682193e-05, 'epoch': 1.6}\n",
      "{'loss': 0.032, 'grad_norm': 0.4900977909564972, 'learning_rate': 4.9364499189781224e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0434, 'grad_norm': 0.34738889336586, 'learning_rate': 4.927426243953252e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0301, 'grad_norm': 0.2778072655200958, 'learning_rate': 4.9178131067507625e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0423, 'grad_norm': 0.714873194694519, 'learning_rate': 4.907612841769407e-05, 'epoch': 1.78}\n",
      "{'loss': 0.0346, 'grad_norm': 0.836617648601532, 'learning_rate': 4.8968279259826536e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0823, 'grad_norm': 2.074953556060791, 'learning_rate': 4.8854609783372014e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0534, 'grad_norm': 1.110138177871704, 'learning_rate': 4.873514759117004e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0333, 'grad_norm': 1.2518432140350342, 'learning_rate': 4.860992169272982e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0486, 'grad_norm': 0.3396487534046173, 'learning_rate': 4.84789624971857e-05, 'epoch': 2.0}\n",
      "{'loss': 0.0207, 'grad_norm': 0.5175408124923706, 'learning_rate': 4.8342301805912814e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0342, 'grad_norm': 0.221979022026062, 'learning_rate': 4.819997280480462e-05, 'epoch': 2.09}\n",
      " 21%|███████▉                              | 471/2240 [22:20<1:23:02,  2.82s/it]"
     ]
    }
   ],
   "source": [
    "for model_instance in model_names: # 3 models\n",
    "\n",
    "    for dataset in dataset_names:\n",
    "\n",
    "        l = [model_instance] + dataset\n",
    "        print(l)\n",
    "        \n",
    "        with open(\"tmp.pkl\", \"wb\") as fh:\n",
    "            \n",
    "            pickle.dump(l, fh)\n",
    "        \n",
    "        print(f\"\\nRunning experiment {l}\")\n",
    "        \n",
    "        %run ./PE_ATC_finetune_v2.ipynb\n",
    "        %cd /Utilisateurs/umushtaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56fee0c-eb0c-4b65-973c-23800f65d27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd041309-0821-4132-ad90-94835c839690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for model_name in model_names: # 3 models\n",
    "\n",
    "#     for dataset in dataset_names:\n",
    "\n",
    "#         l = [model_name] + dataset\n",
    "        \n",
    "#         with open(\"tmp.pkl\", \"wb\") as fh:\n",
    "            \n",
    "#             pickle.dump(l, fh)\n",
    "        \n",
    "#         print(f\"\\nRunning experiment ...\")\n",
    "        \n",
    "#         %run ./PE_ATC_finetune.ipynb\n",
    "#         %cd /Utilisateurs/umushtaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc00d2-8785-4576-9efd-b5398ccd9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c17d7-eaf4-4ffb-81a4-2d3288cd2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../models/PE_LTC_llama-3-8b-Instruct-bnb-4bit/classification_report.pickle\", 'rb') as fh:\n",
    "#     x = pickle.load(fh)\n",
    "#models/PE_LTC_llama-3-70b-Instruct-bnb-4bit/classification_report.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f890218-9867-4b70-a989-923d895fc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"models/PE_LTC_llama-3-70b-Instruct-bnb-4bit/classification_report.pickle\", 'rb') as fh:\n",
    "#     x = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494fa0d-7375-4fb3-bbb9-990887b9252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
