{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oHFCsV0z-Jw"
   },
   "source": [
    "# Finetune PE dataset for LTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr7rB3szzhtx"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "giM74oK1rRIH",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %cd ..\n",
    "# %rm -rf LLaMA-Factory\n",
    "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "# %cd LLaMA-Factory\n",
    "# %ls\n",
    "# !pip install -e .[torch,bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y pydantic\n",
    "# !pip install pydantic==1.10.9 # \n",
    "\n",
    "# !pip uninstall -y gradio\n",
    "# !pip install gradio==3.48.0\n",
    "\n",
    "# !pip uninstall -y bitsandbytes\n",
    "# !pip install --upgrade bitsandbytes\n",
    "\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets\n",
    "# !pip install scikit-learn\n",
    "\n",
    "# Restart kernel afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:    \n",
    "    assert torch.cuda.is_available() is True\n",
    "    \n",
    "except AssertionError:\n",
    "    \n",
    "    print(\"Please set up a GPU before using LLaMA Factory...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeYs5Lz-QJYk"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** MODEL NAME ***\n",
    "\n",
    "# base_model = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "\n",
    "with open(\"tmp.pkl\", \"rb\") as fh:\n",
    "        \n",
    "        l = pickle.load(fh)\n",
    "        base_model = l[0]\n",
    "        train_dataset_name = l[1]\n",
    "        test_dataset_name = l[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "\n",
    "# *** TRAIN DATASET NAME *** #\n",
    "\n",
    "# train_dataset_name = \"PE_LTC_train.json\"\n",
    "train_dataset_file = os.path.join(dataset_dir, train_dataset_name)\n",
    "\n",
    "# *** TEST DATASET NAME *** #\n",
    "\n",
    "# test_dataset_name = \"PE_LTC_test.json\"\n",
    "test_dataset_file = os.path.join(dataset_dir, test_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgR3UFhB0Ifq"
   },
   "source": [
    "## Fine-tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# *** MODEL DIR ***\n",
    "model_name = f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{base_model.split(\"/\")[1]}\"\"\"\n",
    "\n",
    "train_file = os.path.join(os.getcwd(), f\"\"\"cli_files/{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{base_model.split(\"/\")[1]}.json\"\"\")\n",
    "output_dir = os.path.join(os.getcwd(), \"models\", model_name)\n",
    "\n",
    "nb_epochs = 10 # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/cli_files/PE_LTC_llama-3-70b-Instruct-bnb-4bit.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_info_line =  {\n",
    "  \"file_name\": f\"{train_dataset_file}\",\n",
    "  \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\"\n",
    "  }\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': '/Utilisateurs/umushtaq/datasets/PE_LTC_train.json',\n",
       " 'columns': {'prompt': 'instruction', 'query': 'input', 'response': 'output'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_info_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(\"LLaMA-Factory/data/dataset_info.json\", 'r+') as fh:\n",
    "#     file_data = json.load(fh)\n",
    "#     file_data[\"persuasive_essays\"] = dataset_info_line\n",
    "#     fh.seek(0)\n",
    "#     json.dump(file_data, fh, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LLaMA-Factory/data/dataset_info.json\", \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "\n",
    "data[\"persuasive_essays\"] = dataset_info_line\n",
    "\n",
    "with open(\"LLaMA-Factory/data/dataset_info.json\", \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CS0Qk5OR0i4Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = dict(\n",
    "  stage=\"sft\",                           # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "  model_name_or_path=base_model,         # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=\"persuasive_essays\",           # use alpaca and identity datasets\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=output_dir,                 # the path to save LoRA adapters\n",
    "  overwrite_output_dir=True,             # overrides existing output contents\n",
    "  per_device_train_batch_size=2,         # the batch size\n",
    "  gradient_accumulation_steps=4,         # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",            # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=3000,                       # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                    # the learning rate\n",
    "  num_train_epochs=nb_epochs,            # the epochs of training\n",
    "  max_samples=2000,                       # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  quantization_bit=4,                    # use 4-bit QLoRA\n",
    "  loraplus_lr_ratio=16.0,                # use LoRA+ algorithm with lambda=16.0\n",
    "  fp16=True,                             # use float16 mixed precision training\n",
    "  report_to=\"none\"                       # discards wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(args, open(train_file, \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory/\n",
    "!set train_file = train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Utilisateurs/umushtaq/cli_files/PE_LTC_llama-3-70b-Instruct-bnb-4bit.json'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/07/2024 18:43:21 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "06/07/2024 18:43:21 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-07 18:43:21,623 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-07 18:43:21,624 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-07 18:43:21,624 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-07 18:43:21,624 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-07 18:43:22,076 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "06/07/2024 18:43:22 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "06/07/2024 18:43:22 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/datasets/PE_LTC_train.json...\n",
      "Running tokenizer on dataset: 100%|███| 500/500 [00:00<00:00, 903.84 examples/s]\n",
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 14711, 1472, 527, 459, 6335, 304, 14138, 26917, 13, 1472, 527, 2728, 264, 14646, 902, 5727, 5811, 6956, 44910, 555, 366, 1741, 1500, 1741, 29, 9681, 13, 1472, 527, 1101, 2728, 264, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 279, 1376, 25, 18305, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 5850, 61453, 320, 5775, 10807, 320, 396, 705, 2592, 10807, 320, 396, 595, 948, 4718, 3465, 374, 311, 49229, 1855, 6857, 315, 5552, 5811, 6956, 304, 279, 1160, 439, 3060, 330, 8075, 1, 477, 330, 29702, 3343, 1472, 2011, 471, 264, 1160, 315, 12976, 4595, 304, 2768, 4823, 3645, 25, 5324, 23013, 9962, 794, 510, 23013, 1857, 320, 496, 705, 12976, 1857, 320, 496, 705, 61453, 12976, 1857, 320, 496, 7400, 633, 14711, 5810, 374, 279, 14646, 1495, 25, 366, 16816, 29, 12540, 4236, 387, 15972, 311, 20874, 477, 311, 47903, 949, 694, 16816, 397, 14711, 8586, 374, 279, 1160, 315, 13840, 315, 5552, 5811, 6956, 304, 420, 14646, 25, 3132, 128009, 128006, 78191, 128007, 271, 5018, 23013, 9962, 794, 3132, 92, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### You are an expert in Argument Mining. You are given a paragraph which contains argument components enclosed by <AC></AC> tags. You are also given a list of pairs of related argument components in the form: [(target AC (int), source AC (int)), (target AC (int), source AC (int)),..., (target AC (int), source AC (int))]. Your task is to classify each pair of related argument components in the list as either \"Support\" or \"Attack\". You must return a list of relation types in following JSON format: {\"relation_types\": [relation_type (str), relation_type (str),..., relation_type (str)]}\n",
      "\n",
      "### Here is the paragraph text: <topic> Should students be taught to compete or to cooperate? </topic>\n",
      "###Here is the list of pairs of related argument components in this paragraph: []<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"relation_types\": []}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 23013, 9962, 794, 3132, 92, 128009]\n",
      "labels:\n",
      "{\"relation_types\": []}<|eot_id|>\n",
      "/Utilisateurs/umushtaq/.conda/envs/llama-factory-notebook/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-07 18:43:23,361 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-07 18:43:23,362 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/llama-3-70b-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "06/07/2024 18:43:23 - INFO - llamafactory.model.utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "[WARNING|quantization_config.py:393] 2024-06-07 18:43:23,530 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3474] 2024-06-07 18:43:23,533 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-07 18:43:23,553 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-07 18:43:23,554 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [06:01<00:00, 60.25s/it]\n",
      "[INFO|modeling_utils.py:4280] 2024-06-07 18:49:26,522 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-07 18:49:26,522 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-70b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-07 18:49:26,686 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--llama-3-70b-Instruct-bnb-4bit/snapshots/df8fab159e49c133bb4f074baeff4b303614b6d1/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-07 18:49:26,687 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "06/07/2024 18:49:27 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/07/2024 18:49:27 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/07/2024 18:49:27 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/07/2024 18:49:27 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/07/2024 18:49:27 - INFO - llamafactory.model.utils.misc - Found linear modules: gate_proj,q_proj,k_proj,up_proj,down_proj,o_proj,v_proj\n",
      "06/07/2024 18:49:28 - INFO - llamafactory.model.loader - trainable params: 103546880 || all params: 70657253376 || trainable%: 0.1465\n",
      "[INFO|trainer.py:641] 2024-06-07 18:49:28,906 >> Using auto half precision backend\n",
      "06/07/2024 18:49:28 - WARNING - llamafactory.extras.callbacks - Previous trainer log in this folder will be deleted.\n",
      "06/07/2024 18:49:29 - INFO - llamafactory.train.utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "[INFO|trainer.py:2078] 2024-06-07 18:49:29,237 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-07 18:49:29,237 >>   Num examples = 500\n",
      "[INFO|trainer.py:2080] 2024-06-07 18:49:29,237 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:2081] 2024-06-07 18:49:29,237 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2084] 2024-06-07 18:49:29,237 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-07 18:49:29,237 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2086] 2024-06-07 18:49:29,237 >>   Total optimization steps = 620\n",
      "[INFO|trainer.py:2087] 2024-06-07 18:49:29,248 >>   Number of trainable parameters = 103,546,880\n",
      "{'loss': 0.4727, 'grad_norm': 1.5419628620147705, 'learning_rate': 4.838709677419355e-06, 'epoch': 0.16}\n",
      "{'loss': 0.024, 'grad_norm': 0.0015839508268982172, 'learning_rate': 1.2903225806451613e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0239, 'grad_norm': 0.28980955481529236, 'learning_rate': 2.0967741935483873e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0261, 'grad_norm': 1.6246081590652466, 'learning_rate': 2.9032258064516133e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0175, 'grad_norm': 0.5725317001342773, 'learning_rate': 3.7096774193548386e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0118, 'grad_norm': 0.9558072090148926, 'learning_rate': 4.516129032258064e-05, 'epoch': 0.96}\n",
      "{'loss': 0.021, 'grad_norm': 0.15179303288459778, 'learning_rate': 4.999366067542833e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0017, 'grad_norm': 0.0004760722513310611, 'learning_rate': 4.992238019068718e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0119, 'grad_norm': 0.00012888578930869699, 'learning_rate': 4.977212170395598e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1891, 'grad_norm': nan, 'learning_rate': 4.9595362359820727e-05, 'epoch': 1.6}\n",
      "{'loss': 0.417, 'grad_norm': 3.8074803352355957, 'learning_rate': 4.943003080687035e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0056, 'grad_norm': 0.0009985173819586635, 'learning_rate': 4.909264109724853e-05, 'epoch': 1.92}\n",
      "{'loss': 0.0046, 'grad_norm': 1.301408290863037, 'learning_rate': 4.867890279483717e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0188, 'grad_norm': 0.15575237572193146, 'learning_rate': 4.81901270193697e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0087, 'grad_norm': 0.05835619196295738, 'learning_rate': 4.7627862681258037e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0059, 'grad_norm': 0.00019673860515467823, 'learning_rate': 4.6993891573159006e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0037, 'grad_norm': 0.001135112950578332, 'learning_rate': 4.629022272354637e-05, 'epoch': 2.72}\n",
      "{'loss': 0.0075, 'grad_norm': 0.00048528832849115133, 'learning_rate': 4.551908603018191e-05, 'epoch': 2.88}\n",
      "{'loss': 0.0118, 'grad_norm': 0.11513584107160568, 'learning_rate': 4.468292519366071e-05, 'epoch': 3.04}\n",
      "{'loss': 0.0115, 'grad_norm': 0.007922114804387093, 'learning_rate': 4.378438997342409e-05, 'epoch': 3.2}\n",
      "{'loss': 3.1194, 'grad_norm': 1.0521535873413086, 'learning_rate': 4.3119819680728e-05, 'epoch': 3.36}\n",
      " 34%|████████████▋                        | 212/620 [1:32:12<2:58:35, 26.26s/it]"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train $train_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVNaC-xS5N40"
   },
   "source": [
    "## Inference on the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh8H9A_25SF9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %cd /content/LLaMA-Factory/\n",
    "\n",
    "args = dict(\n",
    "  model_name_or_path=base_model, # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=output_dir,            # load the saved LoRA adapters\n",
    "  template=\"llama3\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(test_dataset_file, \"r+\") as fh:\n",
    "    test_dataset = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prompts = []\n",
    "test_grounds = []\n",
    "\n",
    "for sample in test_dataset:\n",
    "    test_prompts.append(\"\\nUser:\" + sample[\"instruction\"] + sample[\"input\"])\n",
    "    test_grounds.append(sample[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "\n",
    "for prompt in tqdm(test_prompts):\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = \"\"\n",
    "    \n",
    "    for new_text in model.stream_chat(messages):\n",
    "        #print(new_text, end=\"\", flush=True)\n",
    "        response += new_text\n",
    "        #print()\n",
    "    test_predictions.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next(model.engine.model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, f\"\"\"PE_LTC_results_{nb_epochs}.pickle\"\"\"), 'wb') as fh:\n",
    "    results_d = {\"ground_truths\": test_grounds,\n",
    "                 \"predictions\": test_predictions    \n",
    "        \n",
    "    }\n",
    "    pickle.dump(results_d, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = 'models/PE_LTC_essay_wo_tags_llama-3-8b-Instruct-bnb-4bit'\n",
    "# nb_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(output_dir, f\"\"\"PE_LTC_results_{nb_epochs}.pickle\"\"\"), \"rb\") as fh:\n",
    "        \n",
    "        results = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ground_truths': ['{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}',\n",
       "  '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'],\n",
       " 'predictions': [{'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '{\"relation_types\": [\"Support\", \"Support\", \"Support\", \"Support\", \"Support\", \"Support\"]}'}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = results[\"ground_truths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = results[\"predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grounds), len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = [json.loads(x)[\"relation_types\"] for x in grounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [x[\"content\"] for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [json.loads(x)[\"relation_types\"] for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support'],\n",
       " ['Support', 'Support', 'Support', 'Support', 'Support', 'Support']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(x,y) in enumerate(zip(grounds, preds)):\n",
    "    if len(x) != len(y):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opposite(relation_type):\n",
    "    \n",
    "    if relation_type == \"Support\":\n",
    "        return \"Attack\"\n",
    "    else:\n",
    "        return \"Support\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_preds(grounds, preds):\n",
    "\n",
    "    l1, l2 = len(preds), len(grounds)\n",
    "    if l1 < l2:\n",
    "        diff = l2 - l1\n",
    "        preds = preds + [opposite(x) for x in grounds[l1:]]\n",
    "    else:\n",
    "        preds = preds[:l2]\n",
    "        \n",
    "    return preds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(x,y) in enumerate(zip(grounds, preds)):\n",
    "    \n",
    "    if len(x) != len(y):\n",
    "        \n",
    "        preds[i] = harmonize_preds(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTC_preds = [item for row in preds for item in row]\n",
    "LTC_grounds = [item for row in grounds for item in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check: \n",
    "len(LTC_preds) == len(LTC_grounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Support      1.000     1.000     1.000       480\n",
      "\n",
      "    accuracy                          1.000       480\n",
      "   macro avg      1.000     1.000     1.000       480\n",
      "weighted avg      1.000     1.000     1.000       480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(LTC_grounds, LTC_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"\"\"{output_dir}/classification_report.pickle\"\"\", 'wb') as fh:\n",
    "    \n",
    "    pickle.dump(classification_report(LTC_grounds, LTC_preds, output_dict=True), fh)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "186 correct 0.2 epochs, with val separated\n",
    "287 correct 10 epochs, with val separated\n",
    "287 correct 10 epochs, val not separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
